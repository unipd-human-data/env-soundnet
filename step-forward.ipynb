{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unipd-human-data/env-soundnet/blob/main/step-forward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d506f58b",
      "metadata": {
        "id": "d506f58b"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f23e23c",
      "metadata": {
        "id": "0f23e23c"
      },
      "source": [
        "Installazione librerie e setup ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "71cc41d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71cc41d9",
        "outputId": "c1d3a580-f629-408c-da8b-506bc178ac36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.4-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading snntorch-0.9.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo, snntorch, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 snntorch-0.9.4 torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install librosa pydub torchaudio snntorch tqdm matplotlib seaborn torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "daaf4687",
      "metadata": {
        "id": "daaf4687"
      },
      "outputs": [],
      "source": [
        "# Libreria standard\n",
        "import os                               # file e cartelle\n",
        "import numpy as np                      # operazioni matriciali, audio e spet sono matrici\n",
        "import pandas as pd                     # leggere e gestire tabelle come esc50.csv\n",
        "import matplotlib.pyplot as plt         # grafici std\n",
        "import seaborn as sns                   # grafici statistici più belli\n",
        "\n",
        "# Audio\n",
        "import librosa                          # per audio in python, calcola features ecc\n",
        "import librosa.display                  # visualizzare spettrogrammi\n",
        "import torchaudio                       # audio di pytorch, usata per pipeline integrata con PyTorch\n",
        "import torchaudio.transforms as T       # Moduli per convertire audio in MelSpectogram o trasfromazioni\n",
        "from pydub import AudioSegment          # gestire audio a livello più \"umano\"\n",
        "\n",
        "# Deep learning e SNN\n",
        "import torch                            # costruzioni reti neurali\n",
        "import snntorch as snn                  # estensione pytorch per SNN\n",
        "import snntorch.functional as SF\n",
        "from snntorch import spikegen           # converte input in spike train\n",
        "from snntorch import spikeplot as splt\n",
        "import tensorflow as tf                 # costruzione e training di modelli di ml e dl\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchinfo import summary\n",
        "\n",
        "# Altri\n",
        "from tqdm import tqdm                   # aggiungere barre di progresso ai loop, quanto manca al caricamento audio\n",
        "from torch.utils.data import Dataset    # Dataset class\n",
        "from torch.utils.data import DataLoader # DataLoader class\n",
        "from enum import Enum\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7a2bbc87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a2bbc87",
        "outputId": "aa330d69-9ae1-4532-ab68-e60089ace0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU disponibile: []\n",
            "Userai: cpu\n"
          ]
        }
      ],
      "source": [
        "print(\"GPU disponibile:\", tf.config.list_physical_devices('GPU'))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Userai:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba72922",
      "metadata": {
        "id": "dba72922"
      },
      "source": [
        "Collegamento al drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a3ff7e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "a3ff7e7b",
        "outputId": "9cee9cdf-e66f-4b72-e83c-91876c26501d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6536d0c",
      "metadata": {
        "id": "b6536d0c"
      },
      "source": [
        "## Uploading CSV of ESC-50 metadata. Relocation of audio files in the ESC-50 and ESC-10 folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "862b23d3",
      "metadata": {
        "id": "862b23d3"
      },
      "outputs": [],
      "source": [
        "esc_50_df = pd.read_csv(\"/content/drive/MyDrive/HumanData/ESC-50-master/meta/esc50.csv\")\n",
        "\n",
        "def relocate_files(file_name, category, is_esc10):\n",
        "  src = f\"/content/drive/MyDrive/HumanData/ESC-50-master/audio/{file_name}\"\n",
        "  esc50_dest_folder = f\"/content/drive/MyDrive/HumanData/ESC-50/{category}\"\n",
        "  esc10_dest_folder = f\"/content/drive/MyDrive/HumanData/ESC-10/{category}\"\n",
        "\n",
        "  # Ensure destination folder exists, creation folders\n",
        "  os.makedirs(esc50_dest_folder, exist_ok=True)\n",
        "  if is_esc10:\n",
        "    os.makedirs(esc10_dest_folder, exist_ok=True)\n",
        "\n",
        "  dest_esc50 = os.path.join(esc50_dest_folder, file_name)\n",
        "  dest_esc10 = os.path.join(esc10_dest_folder, file_name)\n",
        "\n",
        "  # Check if the file already exists in the destination\n",
        "  if not os.path.exists(dest_esc50):\n",
        "    shutil.copy(src, dest_esc50)\n",
        "    print(f\"Moved {file_name} to {dest_esc50}\")\n",
        "  else:\n",
        "    print(f\"File '{file_name}' already exists in '{dest_esc50}', skipping...\")\n",
        "\n",
        "  if not os.path.exists(dest_esc10):\n",
        "    if is_esc10:\n",
        "      shutil.copy(src, dest_esc10)\n",
        "      print(f\"Moved {file_name} to {dest_esc10}\")\n",
        "  else:\n",
        "    print(f\"File '{file_name}' already exists in '{dest_esc10}', skipping...\")\n",
        "\n",
        "\n",
        "#esc_50_df.apply(lambda row: relocate_files(row['filename'], row['category'], row['esc10']), axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956036dd",
      "metadata": {
        "id": "956036dd"
      },
      "outputs": [],
      "source": [
        "%ls /content/drive/MyDrive/HumanData/ESC-10/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6566fde2",
      "metadata": {
        "id": "6566fde2"
      },
      "source": [
        "## Processare tutti i file audio di ESC10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfdcf3ff",
      "metadata": {
        "id": "cfdcf3ff"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/HumanData/ESC-10'\n",
        "SAMPLE_RATE = 44100  # non fare downsampling\n",
        "N_MELS = 128\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 512\n",
        "DURATION = 5.0       # lunghezza standard (secondi)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "class ESCLabels(Enum):\n",
        "    chainsaw = 0\n",
        "    clock_tick = 1\n",
        "    crackling_fire = 2\n",
        "    crying_baby = 3\n",
        "    dog = 4\n",
        "    helicopter = 5\n",
        "    rain = 6\n",
        "    rooster = 7\n",
        "    sea_waves = 8\n",
        "    sneezing = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c0e7bc0",
      "metadata": {
        "id": "7c0e7bc0"
      },
      "source": [
        "- Se un audio ha una frequenza di campionamento di 44.1 kHz, significa che ogni secondo è rappresentato da 44.100 campioni (numeri). Più alta è la frequenza, più dettagliato sarà il suono (fino a un certo limite). 44.1 kHz è sufficiente per rappresentare tutte le frequenze udibili dall’orecchio umano (fino a ~20 kHz), secondo il teorema di Nyquist.\n",
        "\n",
        "- Trim + Normalizzazione: rimuovo silenzio per non sprecare tempo computazionale su dati inutili. Scalare il segnale in [-1,1] in modo da rimuovere il bias di ampiezza tra clip (evita che la rete impari \"volume=classe\")\n",
        "\n",
        "- Con la normalizzazione vado a modificare il range dell'ampiezza, cioè quanto forte o debole è il segnale"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15d41c93",
      "metadata": {
        "id": "15d41c93"
      },
      "source": [
        "per ora non trimmo i silenzio --> possibile ablation study con e senza trim\n",
        "\n",
        "Perché NON usare trim:\n",
        "Mantieni la coerenza temporale tra le clip\n",
        "→ fondamentale per SNN e spike encoding, che sono sensibili alla sequenza temporale dei pattern.\n",
        "\n",
        "Eviti disallineamenti introdotti da librosa.effects.trim\n",
        "→ anche 0.2 secondi di shift possono alterare l’input percepito dal modello.\n",
        "\n",
        "Se usi segmenti (chunk) fissi via timesteps=50,\n",
        "→ i segmenti devono sempre partire dallo stesso punto temporale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f197fc",
      "metadata": {
        "id": "09f197fc"
      },
      "outputs": [],
      "source": [
        "def load_audio_file(file_path, sr=SAMPLE_RATE, duration=5.0, top_db = 30):\n",
        "    y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
        "    #y, _ = librosa.effects.trim(y, top_db=top_db)           # elimina silence iniziale e finale\n",
        "    y = librosa.util.normalize(y)           # normalizzazione RMS\n",
        "    #if len(y) < int(sr * duration):\n",
        "    #    padding = int(sr * duration) - len(y)\n",
        "    #    y = np.pad(y, (0, padding))         # padding se troppo corto\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5059fc0",
      "metadata": {
        "id": "a5059fc0"
      },
      "source": [
        "controllo di quanto silenzio toglie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85be3e9a",
      "metadata": {
        "id": "85be3e9a"
      },
      "outputs": [],
      "source": [
        "# total_trimmed_start = 0.0\n",
        "# total_trimmed_end = 0.0\n",
        "# clip_count = 0\n",
        "\n",
        "# for label in sorted(os.listdir(BASE_PATH)):\n",
        "#     class_path = os.path.join(BASE_PATH, label)\n",
        "#     for file in os.listdir(class_path):\n",
        "#         if file.endswith(\".wav\"):\n",
        "#             file_path = os.path.join(class_path, file)\n",
        "\n",
        "#             # Carica audio\n",
        "#             y, sr = librosa.load(file_path, sr=44100, duration=5.0)\n",
        "\n",
        "#             # Applica trim\n",
        "#             y_trimmed, index = librosa.effects.trim(y, top_db=30)\n",
        "#             start_sample, end_sample = index\n",
        "\n",
        "#             # Converti in secondi\n",
        "#             start_sec = start_sample / sr\n",
        "#             end_sec = (len(y) - end_sample) / sr\n",
        "\n",
        "#             total_trimmed_start += start_sec\n",
        "#             total_trimmed_end += end_sec\n",
        "#             clip_count += 1\n",
        "\n",
        "#             print(f\"{file} → Trim: start={start_sec:.2f}s, end={end_sec:.2f}s\")\n",
        "\n",
        "# # Statistiche finali\n",
        "# print(\"\\n--- STATISTICHE TOTALI ---\")\n",
        "# print(f\"Clip analizzati: {clip_count}\")\n",
        "# print(f\"Silenzio medio INIZIALE tagliato: {total_trimmed_start / clip_count:.2f} s\")\n",
        "# print(f\"Silenzio medio FINALE tagliato: {total_trimmed_end / clip_count:.2f} s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65899e8f",
      "metadata": {
        "id": "65899e8f"
      },
      "outputs": [],
      "source": [
        "X_audio = []\n",
        "y_labels = []\n",
        "labels = sorted(os.listdir(BASE_PATH))  # lista classi ordinate\n",
        "print(labels, \"\\n\")\n",
        "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "for label in tqdm(labels, desc=\"Caricamento Audio\"):\n",
        "    class_path = os.path.join(BASE_PATH, label)\n",
        "    for file in os.listdir(class_path):\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(class_path, file)\n",
        "            y = load_audio_file(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "            X_audio.append(y)\n",
        "            y_labels.append(label_to_index[label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f271e077",
      "metadata": {
        "id": "f271e077"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X_audio, y_labels, test_size=0.4, random_state=42, stratify=y_labels)\n",
        "\n",
        "# Poi dividi temp in val e test (50/50 => 15% ciascuno)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8dd71bb",
      "metadata": {
        "id": "c8dd71bb"
      },
      "source": [
        "## NAA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c2e6ff",
      "metadata": {
        "id": "77c2e6ff"
      },
      "outputs": [],
      "source": [
        "def center_crop(signal, target_len):\n",
        "    if len(signal) < target_len:\n",
        "        pad_left = (target_len - len(signal)) // 2\n",
        "        pad_right = target_len - len(signal) - pad_left\n",
        "        return np.pad(signal, (pad_left, pad_right), mode='constant')\n",
        "    else:\n",
        "        start = (len(signal) - target_len) // 2\n",
        "        return signal[start:start + target_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eca2f9c",
      "metadata": {
        "id": "9eca2f9c"
      },
      "outputs": [],
      "source": [
        "def naa(y, sr):\n",
        "    augmented = []\n",
        "    target_len = int(sr * 5.0)\n",
        "\n",
        "    # Originale\n",
        "    augmented.append(center_crop(y, target_len))\n",
        "\n",
        "    # Pitch shift\n",
        "    augmented.append(center_crop(librosa.effects.pitch_shift(y, sr=sr, n_steps=+2), target_len))\n",
        "    augmented.append(center_crop(librosa.effects.pitch_shift(y, sr=sr, n_steps=-2), target_len))\n",
        "\n",
        "    # Time stretch\n",
        "    for rate in [0.7, 1.2]:\n",
        "        y_stretched = librosa.effects.time_stretch(y, rate=rate)\n",
        "        augmented.append(center_crop(y_stretched, target_len))\n",
        "\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d7130a",
      "metadata": {
        "id": "15d7130a"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_aug = []\n",
        "y_train_aug = []\n",
        "\n",
        "for i in tqdm(range(len(X_train)), desc=\"NAA\"):\n",
        "    original_audio = X_train[i]\n",
        "    label = y_train[i]\n",
        "\n",
        "    # Applica la tua funzione naa → restituisce 5 versioni (incluso l'originale)\n",
        "    augmented_audios = naa(original_audio, sr=SAMPLE_RATE)\n",
        "\n",
        "    # Aggiungi tutte le versioni alla lista finale\n",
        "    X_train_aug.extend(augmented_audios)\n",
        "    y_train_aug.extend([label] * len(augmented_audios))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cbe717",
      "metadata": {
        "id": "12cbe717"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calcola le lunghezze di tutti gli audio\n",
        "lengths = [len(x) for x in X_train_aug]\n",
        "\n",
        "# Trova il minimo e il massimo\n",
        "min_len = min(lengths)\n",
        "max_len = max(lengths)\n",
        "\n",
        "# Converti in secondi (facoltativo)\n",
        "min_sec = min_len / SAMPLE_RATE\n",
        "max_sec = max_len / SAMPLE_RATE\n",
        "\n",
        "print(f\"Audio più corto: {min_len} samples ({min_sec:.2f} s)\")\n",
        "print(f\"Audio più lungo: {max_len} samples ({max_sec:.2f} s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c76e9d0c",
      "metadata": {
        "id": "c76e9d0c"
      },
      "source": [
        "## Converti in log-Mel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371cd293",
      "metadata": {
        "id": "371cd293"
      },
      "source": [
        "- power_to_db(..., ref=np.max) normalizza ogni spettrogramma rispetto al suo massimo → rende il contrasto tra le bande più utile per la rete. Misura quanto ogni frequenza è forte rispetto al picco. Ha senso np.max perchè ho già normalizzato l'audio a [-1,1] quindi non hai una loudness assoluta. Per la classificazione di suoni ambientali il pattern spettrale è più importante del volume, voglio che ogni clip sia trattato in modo coerente (garantisce scale relative)\n",
        "\n",
        "- power_to_db(..., ref=np.max) converte il Mel-spectrogramma da potenza lineare (espressa in unità numeriche) a decibel (scala logaritmica)\n",
        "\n",
        "- il risultato più intenso avrà valore 0 dB. -80 invece significa “tutto ciò che è almeno 10⁸ volte più debole del valore massimo, consideralo praticamente silenzio”. La scala è negativa perchè stai calcolando il logaritmo del rapporto tra ogni valore del Mel-spect e il valore massimo di quel mel-spect\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70b4b89",
      "metadata": {
        "id": "c70b4b89"
      },
      "outputs": [],
      "source": [
        "def preprocess_audio(y, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
        "  #Calcolare Mel-spectrogram\n",
        "  y_mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "  #Convertire in Log\n",
        "  y_mel = librosa.power_to_db(y_mel, ref=np.max)    #calcola i db rispetto al valore massimo nel Mel-spect.\n",
        "\n",
        "  return y_mel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eecaed7",
      "metadata": {
        "id": "7eecaed7"
      },
      "source": [
        "Clipping dei frame troppo silenziosi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cc1f37",
      "metadata": {
        "id": "d3cc1f37"
      },
      "outputs": [],
      "source": [
        "def clip_silent_frames(mel, threshold_db=-70.0):\n",
        "    silent_mask = np.all(mel < threshold_db, axis=0)\n",
        "    mel[:, silent_mask] = threshold_db  # o np.mean(mel)\n",
        "    return mel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b6978c",
      "metadata": {
        "id": "82b6978c"
      },
      "outputs": [],
      "source": [
        "def batch_logmel(X, sr=SAMPLE_RATE):\n",
        "    mel_list = []\n",
        "    for x in tqdm(X, desc=\"Log-Mel\"):\n",
        "        mel = preprocess_audio(x, sr=sr)\n",
        "        # mel = clip_silent_frames(mel)\n",
        "        mel_list.append(mel)\n",
        "    return mel_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf9f8ff",
      "metadata": {
        "id": "0cf9f8ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_mel = batch_logmel(X_train_aug)\n",
        "X_val_mel = batch_logmel(X_val)\n",
        "X_test_mel = batch_logmel(X_test)\n",
        "\n",
        "\"\"\"\n",
        "X_train_mel = batch_logmel(X_train)\n",
        "X_val_mel = batch_logmel(X_val)\n",
        "X_test_mel = batch_logmel(X_test)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67cd6fc8",
      "metadata": {
        "id": "67cd6fc8"
      },
      "outputs": [],
      "source": [
        "print(\"Train set:\", len(X_train_mel))\n",
        "print(\"Val set:\", len(X_val_mel))\n",
        "print(\"Test set:\", len(X_test_mel))\n",
        "\n",
        "print(\"Shape primo sample train:\", X_train_mel[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a5ae35",
      "metadata": {
        "id": "58a5ae35"
      },
      "source": [
        "visualizzo logmel spect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64d5aba",
      "metadata": {
        "id": "b64d5aba"
      },
      "outputs": [],
      "source": [
        "def show_mel(mel, title=\"Log-Mel Spectrogram\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mel, sr=SAMPLE_RATE, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualizza un esempio a caso dal training set\n",
        "sample_idx = 32\n",
        "show_mel(X_train_mel[sample_idx], title=\"Esempio dal training set\")\n",
        "print(\"Class: \", ESCLabels(y_train[sample_idx]).name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "252635d7",
      "metadata": {
        "id": "252635d7"
      },
      "outputs": [],
      "source": [
        "all_vals = np.concatenate([mel.flatten() for mel in X_train_mel])\n",
        "print(\"Min:\", np.min(all_vals))\n",
        "print(\"Max:\", np.max(all_vals))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd6e51b",
      "metadata": {
        "id": "ecd6e51b"
      },
      "source": [
        "Padding per uniformare a 450"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb262841",
      "metadata": {
        "id": "fb262841"
      },
      "outputs": [],
      "source": [
        "def pad_to_multiple_of(mel, multiple=50, value=-80.0):\n",
        "    # Pads mel spectrogram along time axis to make time frames a multiple of 'multiple'\n",
        "    current_len = mel.shape[1]\n",
        "    target_len = ((current_len + multiple - 1) // multiple) * multiple\n",
        "    pad_width = target_len - current_len\n",
        "    return np.pad(mel, ((0, 0), (0, pad_width)), mode='constant', constant_values=value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "553766d6",
      "metadata": {
        "id": "553766d6"
      },
      "outputs": [],
      "source": [
        "# Applica padding a multiplo di 50 (compatibile con timesteps=50)\n",
        "X_train_mel = [pad_to_multiple_of(mel, multiple=50) for mel in X_train_mel]\n",
        "X_val_mel = [pad_to_multiple_of(mel, multiple=50) for mel in X_val_mel]\n",
        "X_test_mel = [pad_to_multiple_of(mel, multiple=50) for mel in X_test_mel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca16765",
      "metadata": {
        "id": "4ca16765"
      },
      "outputs": [],
      "source": [
        "print(\"Train shape esempio:\", X_train_mel[0].shape)\n",
        "print(\"Val shape esempio:\", X_val_mel[0].shape)\n",
        "print(\"Test shape esempio:\", X_test_mel[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6288b34f",
      "metadata": {
        "id": "6288b34f"
      },
      "source": [
        "## DATA AUG TAA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04173e0b",
      "metadata": {
        "id": "04173e0b"
      },
      "source": [
        "applicare taa dopo normalizzazione perchè imagedatagenerator lavora bene su [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67c0034d",
      "metadata": {
        "id": "67c0034d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "taa_generator = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.25,\n",
        "    shear_range=0.3,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c96366d",
      "metadata": {
        "id": "8c96366d"
      },
      "outputs": [],
      "source": [
        "X_train_mel_np = np.array(X_train_mel)\n",
        "X_val_mel_np = np.array(X_val_mel)\n",
        "X_test_mel_np = np.array(X_test_mel)\n",
        "\n",
        "if len(X_train_mel_np.shape) == 3:\n",
        "    X_train_mel_np = X_train_mel_np[..., np.newaxis]  # (N, 64, 431, 1)\n",
        "\n",
        "y_train_np = np.array(y_train_aug)  # Etichette corrispondenti\n",
        "# y_train_np = np.array(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df802dc6",
      "metadata": {
        "id": "df802dc6"
      },
      "source": [
        "### Normalizzazione:\n",
        "i modelli funzionano meglio con input compresi tra 0 e 1.\n",
        "0 = silenzio\n",
        "1 = banda più forte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8ecc11",
      "metadata": {
        "id": "9c8ecc11"
      },
      "outputs": [],
      "source": [
        "X_train_mel_np = (X_train_mel_np + 80.0) / 80.0\n",
        "X_val_mel_np = (X_val_mel_np + 80.0) / 80.0\n",
        "X_test_mel_np = (X_test_mel_np + 80.0) / 80.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5748799f",
      "metadata": {
        "id": "5748799f"
      },
      "source": [
        "### TAA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1541cff",
      "metadata": {
        "id": "f1541cff"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "X_taa = []\n",
        "y_taa = []\n",
        "\n",
        "augmentations_per_sample = 4  # puoi aumentare questo numero\n",
        "\n",
        "for i in tqdm(range(len(X_train_mel_np)), desc=\"TAA Augmentation\"):\n",
        "    sample = X_train_mel_np[i]  # shape: (64, max_len, 1)\n",
        "    sample = np.expand_dims(sample, axis=0)  # shape: (1, 64, max_len, 1)\n",
        "\n",
        "    # Genera augmentazioni\n",
        "    gen = taa_generator.flow(sample, batch_size=1)\n",
        "    for _ in range(augmentations_per_sample):\n",
        "        aug_sample = next(gen)[0]  # shape: (64, max_len, 1)\n",
        "        X_taa.append(aug_sample)\n",
        "        y_taa.append(y_train_np[i])\n",
        "\n",
        "# Combina con il training set originale\n",
        "X_train_augmented = np.concatenate([X_train_mel_np, np.array(X_taa)], axis=0)\n",
        "y_train_augmented = np.concatenate([y_train_np, np.array(y_taa)], axis=0)\n",
        "\n",
        "print(\"Nuova shape X:\", X_train_augmented.shape)\n",
        "print(\"Nuova shape y:\", y_train_augmented.shape)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95cde93",
      "metadata": {
        "id": "f95cde93"
      },
      "source": [
        "Converto in tensori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9ebf80",
      "metadata": {
        "id": "5a9ebf80"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "X_train = torch.from_numpy(X_train_augmented).float()\n",
        "X_val = torch.from_numpy(X_val_mel_np).float()\n",
        "X_test = torch.from_numpy(X_test_mel_np).float()\n",
        "y_train = torch.from_numpy(y_train_augmented).long()\n",
        "y_val = torch.from_numpy(np.array(y_val)).long()\n",
        "y_test = torch.from_numpy(np.array(y_test)).long()\n",
        "\"\"\"\n",
        "\n",
        "X_train = torch.from_numpy(X_train_mel_np).float()\n",
        "X_val = torch.from_numpy(X_val_mel_np).float()\n",
        "X_test = torch.from_numpy(X_test_mel_np).float()\n",
        "y_train = torch.from_numpy(y_train_np).long()\n",
        "y_val = torch.from_numpy(np.array(y_val)).long()\n",
        "y_test = torch.from_numpy(np.array(y_test)).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae3ce0f",
      "metadata": {
        "id": "0ae3ce0f"
      },
      "outputs": [],
      "source": [
        "print(\"Shape X_train:\", X_train.shape)\n",
        "print(\"Tipo dati:\", X_train.dtype)\n",
        "print(\"Valori min/max:\", torch.min(X_train), torch.max(X_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c303e10",
      "metadata": {
        "id": "5c303e10"
      },
      "outputs": [],
      "source": [
        "def show_mel_sample(index):\n",
        "    mel = X_train[index].numpy().squeeze()  # converti tensore in NumPy\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mel, sr=SAMPLE_RATE, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar()  # non usare '%+2.0f dB' perché ora i valori sono [0, 1]\n",
        "    plt.title(f\"Log-Mel sample #{index} (normalized)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"Class:\", ESCLabels(y_train[index].item()).name)\n",
        "\n",
        "for i in range(3):\n",
        "    show_mel_sample(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c1f0fe",
      "metadata": {
        "id": "c0c1f0fe"
      },
      "outputs": [],
      "source": [
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"Etichette uniche:\", np.unique(y_train))\n",
        "\n",
        "# Stampa etichetta di un esempio\n",
        "for i in range(3):\n",
        "    print(f\"Esempio {i}: label = {y_train[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16676d80",
      "metadata": {
        "id": "16676d80"
      },
      "source": [
        "# Spike Encoding\n",
        "\n",
        "We define now an helper function to visualize the following ecoded spike trains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0659345e",
      "metadata": {
        "id": "0659345e"
      },
      "outputs": [],
      "source": [
        "def visualize_spike_trains(spike_tensor, sample_idx=0, mel_bin=10):\n",
        "    \"\"\"\n",
        "    Visualizes delta spike encodings with three plots:\n",
        "    1. Full raster plot of all mel bins\n",
        "    2. Single mel bin spike train with vertical lines\n",
        "    3. Density plot showing balance between positive and negative spikes\n",
        "\n",
        "    Args:\n",
        "      spike_tensor : torch.Tensor or list\n",
        "          The delta spike tensor. Expected shape: [batch_size, time_frames, n_mels]\n",
        "          If from DataLoader, expected to be a list containing tensors\n",
        "      sample_idx : int\n",
        "          Index of the sample in the batch to visualize\n",
        "      mel_bin : int\n",
        "          Which mel frequency bin to visualize in the spike train plot\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if input is a list (from DataLoader) or direct tensor\n",
        "    if isinstance(spike_tensor, list):\n",
        "        spikes = spike_tensor[0].squeeze(-1)[sample_idx].detach().cpu().numpy()\n",
        "    elif isinstance(spike_tensor, torch.Tensor):\n",
        "        # If 4D tensor [batch_size, time_frames, n_mels]\n",
        "        if len(spike_tensor.shape) == 3:\n",
        "            spikes = spike_tensor.squeeze(-1)[sample_idx, :, :].detach().cpu().numpy()\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected spike tensor shape: {spike_tensor.shape}\")\n",
        "    else:\n",
        "        raise TypeError(\"spike_tensor must be a torch.Tensor or a list containing tensors\")\n",
        "\n",
        "    if len(spikes.shape) > 2:\n",
        "        spikes = spikes.squeeze()\n",
        "\n",
        "    pos_spikes = (spikes > 0).astype(float)\n",
        "    neg_spikes = (spikes < 0).astype(float)\n",
        "\n",
        "    plt.figure(figsize=(10, 12))\n",
        "\n",
        "    # 1. Raster Plot with Inverted Y-axis\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.imshow(spikes.T, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1, origin='lower')\n",
        "    plt.colorbar(ticks=[-1, 0, 1], label='Spike Type')\n",
        "    plt.title(\"Full Raster Plot\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Mel Bin\")\n",
        "    num_mel_bins = spikes.shape[1]\n",
        "    tick_interval = max(1, num_mel_bins // 7)\n",
        "    plt.yticks(np.arange(0, num_mel_bins, tick_interval))\n",
        "\n",
        "    # 2. Single Mel Bin Spike Train with vlines\n",
        "    plt.subplot(3, 1, 2)\n",
        "    pos_times = np.where(spikes[:, mel_bin] == 1)[0]\n",
        "    neg_times = np.where(spikes[:, mel_bin] == -1)[0]\n",
        "    plt.vlines(pos_times, 0, 1, color='red', linewidth=0.8)\n",
        "    plt.vlines(neg_times, -1, 0, color='blue', linewidth=0.8)\n",
        "    plt.yticks([-1, 0, 1])\n",
        "    plt.ylim(-1.2, 1.2)\n",
        "    plt.title(f\"Spike Train for Mel Bin {mel_bin}\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Spike Value\")\n",
        "    plt.grid(False)\n",
        "    legend_elements = [plt.Line2D([0], [0], color='red', lw=2, label='Positive Spikes'),\n",
        "                      plt.Line2D([0], [0], color='blue', lw=2, label='Negative Spikes')]\n",
        "    plt.legend(handles=legend_elements)\n",
        "\n",
        "    # 3. Density Plot with Legend\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.stackplot(np.arange(spikes.shape[0]),\n",
        "                  pos_spikes.sum(axis=1),\n",
        "                  -neg_spikes.sum(axis=1),\n",
        "                  colors=['red', 'blue'])\n",
        "\n",
        "    plt.legend(['Positive Spikes', 'Negative Spikes'])\n",
        "    plt.title(\"Spike Polarity Balance Over Time\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Net Spike Count\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step Forward"
      ],
      "metadata": {
        "id": "Y_FdEyLKB4Ps"
      },
      "id": "Y_FdEyLKB4Ps"
    },
    {
      "cell_type": "code",
      "source": [
        "class StepForwardDataset(Dataset):\n",
        "\n",
        "  \"\"\"\n",
        "  Dataset class that applies Step Forward encoding to spectrograms.\n",
        "  \"\"\"\n",
        "  def __init__(self, X, y, threshold=0.05):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Tensor of shape [num_samples, n_mels, time_steps, 1]\n",
        "      y: Tensor of labels [num_samples]\n",
        "      threshold: Threshold for Step Forward encoding\n",
        "    \"\"\"\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = self.X[idx].squeeze(-1)  # [n_mels, time_steps]\n",
        "    y = self.y[idx]\n",
        "\n",
        "    spike_train = torch.zeros(\n",
        "        x.shape[1], x.shape[0], dtype=torch.int8\n",
        "    )  # [time_steps, n_mels]\n",
        "\n",
        "    # Apply Step Forward encoding to each mel frequency bin\n",
        "    for mel_idx in range(x.shape[0]):\n",
        "        spikes, _ = self._encode(x[mel_idx])\n",
        "        spike_train[:, mel_idx] = torch.from_numpy(spikes)\n",
        "\n",
        "    return spike_train, y.long()\n",
        "\n",
        "  def _encode(self, input_signal):\n",
        "    \"\"\"\n",
        "    Step Forward encoding algorithm implementation.\n",
        "\n",
        "    Args:\n",
        "        input_signal: 1D array/tensor - input signal to encode\n",
        "        threshold: float - threshold value for spike generation\n",
        "\n",
        "    Returns:\n",
        "        spikes: 1D array - encoded spike train with values {-1, 0, 1}\n",
        "        base_history: 1D array - history of base values (for debugging/analysis)\n",
        "    \"\"\"\n",
        "    # Convert to numpy if torch tensor\n",
        "    if isinstance(input_signal, torch.Tensor):\n",
        "        input_signal = input_signal.numpy()\n",
        "\n",
        "    L = len(input_signal)\n",
        "    spikes = np.zeros(L, dtype=np.int8)\n",
        "    base_history = np.zeros(L)  # Track base evolution\n",
        "\n",
        "    # Initialize base with first input value\n",
        "    base = input_signal[0]\n",
        "    base_history[0] = base\n",
        "\n",
        "    # Process each timestep starting from index 1\n",
        "    for i in range(1, L):\n",
        "        if input_signal[i] > base + self.threshold:\n",
        "            spikes[i] = 1\n",
        "            base = base + self.threshold  # Update base upward\n",
        "        elif input_signal[i] < base - self.threshold:\n",
        "            spikes[i] = -1\n",
        "            base = base - self.threshold  # Update base downward\n",
        "        # else: spikes[i] remains 0\n",
        "\n",
        "        base_history[i] = base\n",
        "\n",
        "    return spikes, base_history\n"
      ],
      "metadata": {
        "id": "dXfh4Z-qB3W2"
      },
      "id": "dXfh4Z-qB3W2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sf_train_dataset = StepForwardDataset(\n",
        "    X_train, y_train,\n",
        "    threshold=0.05\n",
        ")\n",
        "\n",
        "sf_val_dataset = StepForwardDataset(\n",
        "    X_val, y_val,\n",
        "    threshold=0.05\n",
        ")\n",
        "\n",
        "sf_test_dataset = StepForwardDataset(\n",
        "    X_test, y_test,\n",
        "    threshold=0.05\n",
        ")\n",
        "\n",
        "sf_train_dataloader = DataLoader(\n",
        "    dataset=sf_train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "sf_val_dataloader = DataLoader(\n",
        "    dataset=sf_val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "sf_test_dataloader = DataLoader(\n",
        "    dataset=sf_test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "\n",
        "sf_x_batch, sf_y_batch = next(iter(sf_train_dataloader))\n",
        "print(sf_x_batch.shape)"
      ],
      "metadata": {
        "id": "lgViWlyuJaQv"
      },
      "id": "lgViWlyuJaQv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "visualize_spike_trains(spike_tensor=sf_x_batch, sample_idx=sample_idx, mel_bin=54)"
      ],
      "metadata": {
        "id": "Y-tJxXWTL92Z"
      },
      "id": "Y-tJxXWTL92Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_spikes(spike_train):\n",
        "    return spike_train.abs().sum(dim=[1, 2])  # somma spike su tempo e bande mel\n",
        "\n",
        "spike_counts = count_spikes(sf_x_batch)\n",
        "plt.hist(spike_counts.cpu().numpy(), bins=30)\n",
        "plt.title(\"Distribuzione del numero di spike per clip\")\n",
        "plt.xlabel(\"Spike totali\")\n",
        "plt.ylabel(\"Frequenza\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g1MMmEoyQuBF"
      },
      "id": "g1MMmEoyQuBF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_spikes_batch(data):\n",
        "    \"\"\"\n",
        "    Count positive and negative spikes in a single batch\n",
        "\n",
        "    Args:\n",
        "        data: torch.Tensor of shape (batch_size, channels, timesteps) or (batch_size, timesteps)\n",
        "\n",
        "    Returns:\n",
        "        dict with spike counts for this batch\n",
        "    \"\"\"\n",
        "    # Convert to numpy if it's a torch tensor\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.numpy()\n",
        "\n",
        "    # Handle different tensor shapes\n",
        "    if len(data.shape) == 3:  # (batch_size, channels, timesteps)\n",
        "        # Flatten across batch and channels, keep timesteps\n",
        "        flattened = data.reshape(-1, data.shape[-1])\n",
        "    elif len(data.shape) == 2:  # (batch_size, timesteps)\n",
        "        flattened = data\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected data shape: {data.shape}\")\n",
        "\n",
        "    # Count spikes\n",
        "    positive_spikes = (flattened > 0).sum()\n",
        "    negative_spikes = (flattened < 0).sum()\n",
        "    zero_values = (flattened == 0).sum()\n",
        "    total_values = flattened.size\n",
        "\n",
        "    return {\n",
        "        'positive_spikes': int(positive_spikes),\n",
        "        'negative_spikes': int(negative_spikes),\n",
        "        'zero_values': int(zero_values),\n",
        "        'total_values': int(total_values)\n",
        "    }\n",
        "\n",
        "def analyze_complete_dataset(dataloader, dataset_name=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    Analyze spikes across all batches in a dataloader\n",
        "\n",
        "    Args:\n",
        "        dataloader: PyTorch DataLoader\n",
        "        dataset_name: Name for the dataset (for display purposes)\n",
        "\n",
        "    Returns:\n",
        "        dict with complete spike analysis\n",
        "    \"\"\"\n",
        "    print(f\"Analyzing {dataset_name}...\")\n",
        "\n",
        "    total_positive = 0\n",
        "    total_negative = 0\n",
        "    total_zeros = 0\n",
        "    total_values = 0\n",
        "    batch_results = []\n",
        "\n",
        "    # Iterate through all batches\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(tqdm(dataloader, desc=f\"Processing {dataset_name}\")):\n",
        "        batch_counts = count_spikes_batch(X_batch)\n",
        "\n",
        "        # Accumulate totals\n",
        "        total_positive += batch_counts['positive_spikes']\n",
        "        total_negative += batch_counts['negative_spikes']\n",
        "        total_zeros += batch_counts['zero_values']\n",
        "        total_values += batch_counts['total_values']\n",
        "\n",
        "        # Store batch results\n",
        "        batch_results.append({\n",
        "            'batch_idx': batch_idx,\n",
        "            'batch_size': X_batch.shape[0],\n",
        "            **batch_counts\n",
        "        })\n",
        "\n",
        "    # Calculate overall statistics\n",
        "    results = {\n",
        "        'dataset_name': dataset_name,\n",
        "        'total_batches': len(batch_results),\n",
        "        'total_samples': sum([br['batch_size'] for br in batch_results]),\n",
        "        'total_positive_spikes': total_positive,\n",
        "        'total_negative_spikes': total_negative,\n",
        "        'total_zero_values': total_zeros,\n",
        "        'total_values': total_values,\n",
        "        'positive_ratio': total_positive / total_values if total_values > 0 else 0,\n",
        "        'negative_ratio': total_negative / total_values if total_values > 0 else 0,\n",
        "        'zero_ratio': total_zeros / total_values if total_values > 0 else 0,\n",
        "        'batch_results': batch_results\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_dataset_summary(results):\n",
        "    \"\"\"Print a summary of the dataset analysis\"\"\"\n",
        "    print(f\"\\n{results['dataset_name']} Analysis Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total batches: {results['total_batches']}\")\n",
        "    print(f\"Total samples: {results['total_samples']:,}\")\n",
        "    print(f\"Total values analyzed: {results['total_values']:,}\")\n",
        "    print()\n",
        "    print(\"Spike Counts:\")\n",
        "    print(f\"  Positive spikes: {results['total_positive_spikes']:,}\")\n",
        "    print(f\"  Negative spikes: {results['total_negative_spikes']:,}\")\n",
        "    print(f\"  Zero values: {results['total_zero_values']:,}\")\n",
        "    print()\n",
        "    print(\"Spike Ratios:\")\n",
        "    print(f\"  Positive spike ratio: {results['positive_ratio']:.4f} ({results['positive_ratio']*100:.2f}%)\")\n",
        "    print(f\"  Negative spike ratio: {results['negative_ratio']:.4f} ({results['negative_ratio']*100:.2f}%)\")\n",
        "    print(f\"  Zero ratio: {results['zero_ratio']:.4f} ({results['zero_ratio']*100:.2f}%)\")\n",
        "    print()\n",
        "    print(f\"  Spike density: {(results['total_positive_spikes'] + results['total_negative_spikes']) / results['total_values']:.4f}\")\n",
        "\n",
        "\n",
        "results = analyze_complete_dataset(sf_train_dataloader, dataset_name=\"StepForward Train\")\n",
        "print_dataset_summary(results)"
      ],
      "metadata": {
        "id": "VOrZmC7F2DbJ"
      },
      "id": "VOrZmC7F2DbJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "13b70f7e",
      "metadata": {
        "id": "13b70f7e"
      },
      "source": [
        "# SNN Modelling\n",
        "\n",
        "A single batch of data has shape: \\[batch_size, time_frames, n_mels\\]."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, train_loader, valid_loader, accuracy, loss_fn, optimizer,\n",
        "             epochs, patience, path, verbose=True, max_batches=None):\n",
        "    \"\"\"\n",
        "    Optimized training function with:\n",
        "    - Mixed precision training\n",
        "    - Reduced GPU memory usage\n",
        "    - Progress tracking\n",
        "    \"\"\"\n",
        "    train_loss_list, val_loss_list = [], []\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "    counter = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Enable mixed precision training if available\n",
        "    scaler = torch.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Training mode\n",
        "        model.train()\n",
        "        train_loss, train_acc = 0.0, 0.0\n",
        "\n",
        "        for batch_idx, (X, y) in enumerate(tqdm(train_loader, desc=\"Train batches\", leave=False, total=len(train_loader))):\n",
        "            X = X.squeeze().to(device)\n",
        "            y = y.squeeze().long().to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Use mixed precision where available\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    spk_out, _ = model(X.float())\n",
        "                    acc = accuracy(spk_out, y)\n",
        "                    loss = loss_fn(spk_out, y)\n",
        "\n",
        "                # Scale gradients and optimize\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                spk_out, _ = model(X.float())\n",
        "                acc = accuracy(spk_out, y)\n",
        "                loss = loss_fn(spk_out, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            train_acc += acc.item()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Free up memory\n",
        "            del X, y, spk_out, loss, acc\n",
        "\n",
        "        # Calculate average metrics\n",
        "        train_loss_avg = train_loss / len(train_loader)\n",
        "        train_acc_avg = train_acc / len(train_loader)\n",
        "        train_loss_list.append(train_loss_avg)\n",
        "        train_acc_list.append(train_acc_avg)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_acc = 0.0, 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (X, y) in enumerate(valid_loader):\n",
        "                X = X.squeeze().to(device)\n",
        "                y = y.squeeze().long().to(device)\n",
        "\n",
        "                # Forward pass with reduced memory usage\n",
        "                if scaler:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        spk_out, _ = model(X.float())\n",
        "                        acc = accuracy(spk_out, y)\n",
        "                        loss = loss_fn(spk_out, y)\n",
        "                else:\n",
        "                    spk_out, _ = model(X.float())\n",
        "                    acc = accuracy(spk_out, y)\n",
        "                    loss = loss_fn(spk_out, y)\n",
        "\n",
        "                val_acc += acc.item()\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Free up memory\n",
        "                del X, y, spk_out, loss, acc\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss_avg = val_loss / len(valid_loader)\n",
        "        val_acc_avg = val_acc / len(valid_loader)\n",
        "        val_loss_list.append(val_loss_avg)\n",
        "        val_acc_list.append(val_acc_avg)\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_loss_avg < best_val_loss:\n",
        "            best_val_loss = val_loss_avg\n",
        "            counter = 0\n",
        "            if path:\n",
        "                #torch.save(model.state_dict(), path)\n",
        "                pass\n",
        "        else:\n",
        "            counter += 1\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        end_time = time.time()\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs} - {int(end_time-start_time)}s - \"\n",
        "                  f\"loss: {train_loss_avg:.4f} - acc: {train_acc_avg:.4f} - \"\n",
        "                  f\"val_loss: {val_loss_avg:.4f} - val_acc: {val_acc_avg:.4f}\")\n",
        "\n",
        "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
      ],
      "metadata": {
        "id": "eFkPxghpAcGc"
      },
      "id": "eFkPxghpAcGc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score, precision_score, recall_score\n",
        "\n",
        "def evaluate_model_comprehensive(model, dataloader, class_names, device):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation function that computes accuracy, F1, precision, and recall.\n",
        "    Now handles zero-division gracefully and reports classes with no predictions.\n",
        "    Enhanced confusion matrix shows percentages for each cell.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.squeeze().to(device)\n",
        "            y = y.squeeze().long().to(device)\n",
        "\n",
        "            spk_out, _ = model(X.float())\n",
        "            spk_sum = spk_out.sum(dim=0)  # [batch_size, num_classes]\n",
        "            preds = torch.argmax(spk_sum, dim=1)\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(y.cpu())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    # Identify classes never predicted\n",
        "    missing = set(range(len(class_names))) - set(np.unique(all_preds))\n",
        "    if missing:\n",
        "        print(\"WARNING: The following classes were never predicted:\",\n",
        "              [class_names[i] for i in sorted(missing)])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "\n",
        "    # Macro and weighted metrics with zero_division=0\n",
        "    f1_macro    = f1_score(all_labels, all_preds, average='macro',    zero_division=0)\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    precision_macro    = precision_score(all_labels, all_preds, average='macro',    zero_division=0)\n",
        "    precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    recall_macro    = recall_score(all_labels, all_preds, average='macro',    zero_division=0)\n",
        "    recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    # Per-class metrics\n",
        "    f1_per_class        = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    recall_per_class    = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"=\"*60)\n",
        "    print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\\n\")\n",
        "\n",
        "    print(\"MACRO AVERAGES:\")\n",
        "    print(f\"  F1-Score   : {f1_macro:.4f}\")\n",
        "    print(f\"  Precision  : {precision_macro:.4f}\")\n",
        "    print(f\"  Recall     : {recall_macro:.4f}\\n\")\n",
        "\n",
        "    print(\"WEIGHTED AVERAGES:\")\n",
        "    print(f\"  F1-Score   : {f1_weighted:.4f}\")\n",
        "    print(f\"  Precision  : {precision_weighted:.4f}\")\n",
        "    print(f\"  Recall     : {recall_weighted:.4f}\\n\")\n",
        "\n",
        "    print(\"PER-CLASS METRICS:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Class':<15} {'F1':<8} {'Precision':<10} {'Recall':<8}\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"{name:<15} \"\n",
        "              f\"{f1_per_class[i]:<8.4f} \"\n",
        "              f\"{precision_per_class[i]:<10.4f} \"\n",
        "              f\"{recall_per_class[i]:<8.4f}\")\n",
        "\n",
        "    # Detailed report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        target_names=class_names,\n",
        "        digits=4,\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    # Enhanced Confusion Matrix with percentages\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Create percentage matrix (normalize by row - true class)\n",
        "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "    # Handle division by zero (classes with no samples)\n",
        "    cm_percentage = np.nan_to_num(cm_percentage)\n",
        "\n",
        "    # Create the enhanced confusion matrix plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    # Plot 1: Standard confusion matrix with counts\n",
        "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp1.plot(ax=ax1, cmap=\"Blues\", xticks_rotation=45)\n",
        "    ax1.set_title(\"Confusion Matrix - Counts\", fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Plot 2: Enhanced confusion matrix with percentages\n",
        "    im = ax2.imshow(cm_percentage, interpolation='nearest', cmap='Blues')\n",
        "    ax2.figure.colorbar(im, ax=ax2)\n",
        "    ax2.set(xticks=np.arange(cm.shape[1]),\n",
        "            yticks=np.arange(cm.shape[0]),\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            title=\"Confusion Matrix - Percentages (Row-wise)\",\n",
        "            ylabel='True label',\n",
        "            xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment\n",
        "    plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    # Add text annotations with both count and percentage\n",
        "    fmt = '.1f'\n",
        "    thresh = cm_percentage.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            # Display both count and percentage\n",
        "            text = f'{cm[i, j]}\\n({cm_percentage[i, j]:.1f}%)'\n",
        "            ax2.text(j, i, text,\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm_percentage[i, j] > thresh else \"black\",\n",
        "                    fontsize=10, fontweight='bold')\n",
        "\n",
        "    ax2.set_title(\"Confusion Matrix - Counts & Percentages\", fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Additional analysis: Print per-class accuracy breakdown\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PER-CLASS ACCURACY BREAKDOWN\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'True Class':<15} {'Correct':<8} {'Total':<8} {'Accuracy':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        correct = cm[i, i]  # Diagonal elements\n",
        "        total = cm[i, :].sum()  # Total samples for this true class\n",
        "        class_accuracy = correct / total * 100 if total > 0 else 0\n",
        "        print(f\"{name:<15} {correct:<8} {total:<8} {class_accuracy:<10.2f}%\")\n",
        "\n",
        "    # Create a third plot showing only the diagonal accuracy\n",
        "    fig, ax3 = plt.subplots(figsize=(10, 6))\n",
        "    class_accuracies = []\n",
        "    for i in range(len(class_names)):\n",
        "        correct = cm[i, i]\n",
        "        total = cm[i, :].sum()\n",
        "        acc = correct / total * 100 if total > 0 else 0\n",
        "        class_accuracies.append(acc)\n",
        "\n",
        "    bars = ax3.bar(class_names, class_accuracies, color='skyblue', alpha=0.7)\n",
        "    ax3.set_ylabel('Accuracy (%)')\n",
        "    ax3.set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
        "    ax3.set_ylim(0, 100)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, acc in zip(bars, class_accuracies):\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Add horizontal line for overall accuracy\n",
        "    ax3.axhline(y=accuracy*100, color='red', linestyle='--', alpha=0.7,\n",
        "                label=f'Overall Accuracy: {accuracy*100:.1f}%')\n",
        "    ax3.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'precision_macro': precision_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_macro': recall_macro,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_per_class': f1_per_class,\n",
        "        'precision_per_class': precision_per_class,\n",
        "        'recall_per_class': recall_per_class,\n",
        "        'confusion_matrix': cm,\n",
        "        'confusion_matrix_percentage': cm_percentage,\n",
        "        'per_class_accuracy': class_accuracies\n",
        "    }"
      ],
      "metadata": {
        "id": "JWQ3zWKZATys"
      },
      "id": "JWQ3zWKZATys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple SNN"
      ],
      "metadata": {
        "id": "59C_MBwm_nXe"
      },
      "id": "59C_MBwm_nXe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "081ada25",
      "metadata": {
        "id": "081ada25"
      },
      "outputs": [],
      "source": [
        "class SNNClassifier(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_mels,\n",
        "        hidden_sizes,\n",
        "        num_classes,\n",
        "        surr_grad,\n",
        "        learn_thr=True,\n",
        "        learn_beta=True,\n",
        "    ):\n",
        "        super(SNNClassifier, self).__init__()\n",
        "        self.n_mels = n_mels\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.num_classes = num_classes\n",
        "        self.surr_grad = surr_grad\n",
        "        self.learn_thr = learn_thr\n",
        "        self.learn_beta = learn_beta\n",
        "\n",
        "        # Layer 1: Input to Hidden 1\n",
        "        self.fc1 = torch.nn.Linear(n_mels, self.hidden_sizes[0])\n",
        "        self.lif1 = snn.Leaky(\n",
        "            beta=torch.full((self.hidden_sizes[0],), 0.5),\n",
        "            learn_beta=learn_beta,\n",
        "            learn_threshold=learn_thr,\n",
        "            spike_grad=surr_grad,\n",
        "            reset_mechanism=\"zero\",\n",
        "        )\n",
        "\n",
        "        # Layer 2: Hidden 1 to Hidden 2\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_sizes[0], self.hidden_sizes[1])\n",
        "        self.lif2 = snn.Leaky(\n",
        "            beta=torch.full((self.hidden_sizes[1],), 0.5),\n",
        "            learn_beta=learn_beta,\n",
        "            learn_threshold=learn_thr,\n",
        "            spike_grad=surr_grad,\n",
        "            reset_mechanism=\"zero\",\n",
        "        )\n",
        "\n",
        "        # Layer 3: Hidden 2 to Hidden 3\n",
        "        self.fc3 = torch.nn.Linear(self.hidden_sizes[1], self.hidden_sizes[2])\n",
        "        self.lif3 = snn.Leaky(\n",
        "            beta=torch.full((self.hidden_sizes[2],), 0.5),\n",
        "            learn_beta=learn_beta,\n",
        "            learn_threshold=learn_thr,\n",
        "            spike_grad=surr_grad,\n",
        "            reset_mechanism=\"zero\",\n",
        "        )\n",
        "\n",
        "        # Output Layer\n",
        "        self.fc_out = torch.nn.Linear(self.hidden_sizes[2], num_classes)\n",
        "        self.lif_out = snn.Leaky(\n",
        "            beta=torch.full((num_classes,), 0.5),\n",
        "            learn_beta=learn_beta,\n",
        "            learn_threshold=learn_thr,\n",
        "            spike_grad=surr_grad,\n",
        "            reset_mechanism='zero',\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, time_steps, _ = x.shape\n",
        "\n",
        "        # Initialize membrane potentials\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem_out = self.lif_out.init_leaky()\n",
        "\n",
        "        spk_rec = []\n",
        "        mem_rec = []\n",
        "\n",
        "        for step in range(time_steps):\n",
        "            x_t = x[:, step, :]\n",
        "\n",
        "            cur1 = self.fc1(x_t)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "\n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            cur_out = self.fc_out(spk3)\n",
        "            spk_out, mem_out = self.lif_out(cur_out, mem_out)\n",
        "\n",
        "            spk_rec.append(spk_out)\n",
        "            mem_rec.append(mem_out)\n",
        "\n",
        "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c4fdad",
      "metadata": {
        "id": "73c4fdad"
      },
      "outputs": [],
      "source": [
        "hidden_sizes = [128, 128, 128]\n",
        "num_classes = 10\n",
        "\n",
        "snn_classifier = SNNClassifier(\n",
        "    n_mels=N_MELS,\n",
        "    hidden_sizes=hidden_sizes,\n",
        "    num_classes=num_classes,\n",
        "    surr_grad=snn.surrogate.fast_sigmoid(),\n",
        "    learn_thr=True,\n",
        "    learn_beta=True\n",
        ").to(device)\n",
        "\n",
        "summary(snn_classifier, input_size=(32, 1, N_MELS))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple SNN Training"
      ],
      "metadata": {
        "id": "fycHpPFkMZXh"
      },
      "id": "fycHpPFkMZXh"
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 30\n",
        "num_epochs = 100\n",
        "optimizer = torch.optim.Adam(snn_classifier.parameters(), lr=0.001)\n",
        "\n",
        "train_loss, train_acc, val_loss, val_acc = train_fn(\n",
        "    model=snn_classifier,\n",
        "    train_loader=sf_train_dataloader,\n",
        "    valid_loader=sf_val_dataloader,\n",
        "    accuracy=SF.acc.accuracy_rate,\n",
        "    loss_fn=snn.functional.ce_count_loss(),\n",
        "    optimizer=optimizer,\n",
        "    epochs=num_epochs,\n",
        "    patience=patience,\n",
        "    path=\"./\",\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "_kdJpShWMhFl"
      },
      "id": "_kdJpShWMhFl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(np.arange(1, len(train_loss)+1), train_loss, label='Train loss')\n",
        "plt.plot(np.arange(1, len(train_loss)+1), val_loss, label='Val loss')\n",
        "plt.xticks(np.arange(1, len(train_loss)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(alpha=.7)\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(np.arange(1, len(train_acc)+1), train_acc, label='Train accuracy')\n",
        "plt.plot(np.arange(1, len(train_acc)+1), val_acc, label='Val accuracy')\n",
        "plt.xticks(np.arange(1, len(train_acc)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(alpha=.7)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Etj7kHMsMyGm"
      },
      "id": "Etj7kHMsMyGm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\n",
        "    'chainsaw', 'clock_tick', 'crackling_fire', 'crying_baby', 'dog',\n",
        "    'helicopter', 'rain', 'rooster', 'sea_waves', 'sneezing'\n",
        "]\n",
        "\n",
        "results = evaluate_model_comprehensive(snn_classifier, sf_test_dataloader, class_names, device)"
      ],
      "metadata": {
        "id": "CdMVQ0ZTMzN8"
      },
      "id": "CdMVQ0ZTMzN8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional SNN (C-SNN)"
      ],
      "metadata": {
        "id": "NRln91Z9AlQX"
      },
      "id": "NRln91Z9AlQX"
    },
    {
      "cell_type": "code",
      "source": [
        "class C_SNN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        surr_grad,\n",
        "        n_mels: int = 128,\n",
        "        num_classes: int = 10,\n",
        "        conv_channels1: int = 8,\n",
        "        conv_channels2: int = 64,\n",
        "        kernel_size: int = 3,\n",
        "        pool_kernel: int = 2,\n",
        "        dropout_rate: float = 0.15\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.surr_grad = surr_grad\n",
        "\n",
        "        # --- First Spiking Convolutional Block ---\n",
        "        self.conv1 = torch.nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=conv_channels1,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.lif1 = snn.Leaky(\n",
        "            beta=0.9,\n",
        "            threshold=1.0,\n",
        "            learn_beta=True,\n",
        "            learn_threshold=True,\n",
        "            spike_grad=surr_grad\n",
        "        )\n",
        "        self.pool1 = torch.nn.MaxPool1d(kernel_size=pool_kernel)\n",
        "\n",
        "        # --- Fully Connected Layer ---\n",
        "        freq_after_pool = n_mels // pool_kernel\n",
        "        self.flattened_size = conv_channels1 * freq_after_pool\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "        self.fc1 = torch.nn.Linear(self.flattened_size, num_classes, bias=False)\n",
        "        self.lif_out = snn.Leaky(\n",
        "            beta=0.9,\n",
        "            threshold=1.0,\n",
        "            learn_beta=True,\n",
        "            learn_threshold=True,\n",
        "            spike_grad=surr_grad\n",
        "        )\n",
        "\n",
        "    def forward(self, spikes: torch.Tensor):\n",
        "        \"\"\"\n",
        "        spikes: [batch_size, time_steps, n_mels]\n",
        "        Returns:\n",
        "            spk_rec: [time_steps, batch_size, num_classes]\n",
        "            mem_rec: [time_steps, batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        B, T, F = spikes.shape\n",
        "\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem_out = self.lif_out.init_leaky()\n",
        "\n",
        "        spk_rec = []\n",
        "        mem_rec = []\n",
        "\n",
        "        spikes_tm = spikes.permute(1, 0, 2)  # [T, B, F]\n",
        "\n",
        "        for t in range(T):\n",
        "            x_t = spikes_tm[t].unsqueeze(1)  # [B, 1, F]\n",
        "\n",
        "            # --- Conv Block 1 ---\n",
        "            x = self.conv1(x_t)\n",
        "            spk1, mem1 = self.lif1(x, mem1)\n",
        "            x = self.pool1(spk1)\n",
        "\n",
        "            # --- Flatten + FC ---\n",
        "            x = x.view(B, -1)\n",
        "            x = self.dropout(x)\n",
        "            x = self.fc1(x)\n",
        "            spk_out, mem_out = self.lif_out(x, mem_out)\n",
        "\n",
        "            spk_rec.append(spk_out)\n",
        "            mem_rec.append(mem_out)\n",
        "\n",
        "        spk_rec = torch.stack(spk_rec, dim=0)\n",
        "        mem_rec = torch.stack(mem_rec, dim=0)\n",
        "\n",
        "        return spk_rec, mem_rec\n"
      ],
      "metadata": {
        "id": "ykEDj5aGAkjo"
      },
      "id": "ykEDj5aGAkjo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_snn_model = C_SNN(\n",
        "    surr_grad=snn.surrogate.fast_sigmoid(),\n",
        "    n_mels=N_MELS,\n",
        "    num_classes=10,\n",
        "    conv_channels1=8,\n",
        "    conv_channels2=8,\n",
        "    dropout_rate=0.0,\n",
        "    kernel_size=3,\n",
        "    pool_kernel=2\n",
        ").to(device)\n",
        "\n",
        "\n",
        "summary(c_snn_model, input_size=(BATCH_SIZE, 1, N_MELS))"
      ],
      "metadata": {
        "id": "cM5B9mvaEM0r"
      },
      "id": "cM5B9mvaEM0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSNN Training"
      ],
      "metadata": {
        "id": "Qtg8-4YbKYI5"
      },
      "id": "Qtg8-4YbKYI5"
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 30\n",
        "num_epochs = 100\n",
        "optimizer = torch.optim.Adam(c_snn_model.parameters(), lr=0.0001)\n",
        "\n",
        "train_loss, train_acc, val_loss, val_acc = train_fn(\n",
        "    model=c_snn_model,\n",
        "    train_loader=sf_train_dataloader,\n",
        "    valid_loader=sf_val_dataloader,\n",
        "    accuracy=SF.acc.accuracy_rate,\n",
        "    loss_fn=snn.functional.ce_count_loss(),\n",
        "    optimizer=optimizer,\n",
        "    epochs=num_epochs,\n",
        "    patience=patience,\n",
        "    path=\"./\",\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "L_dkQhajHevB"
      },
      "id": "L_dkQhajHevB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(np.arange(1, len(train_loss)+1), train_loss, label='Train loss')\n",
        "plt.plot(np.arange(1, len(train_loss)+1), val_loss, label='Val loss')\n",
        "plt.xticks(np.arange(1, len(train_loss)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(alpha=.7)\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(np.arange(1, len(train_acc)+1), train_acc, label='Train accuracy')\n",
        "plt.plot(np.arange(1, len(train_acc)+1), val_acc, label='Val accuracy')\n",
        "plt.xticks(np.arange(1, len(train_acc)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(alpha=.7)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "class_names = [\n",
        "    'chainsaw', 'clock_tick', 'crackling_fire', 'crying_baby', 'dog',\n",
        "    'helicopter', 'rain', 'rooster', 'sea_waves', 'sneezing'\n",
        "]\n",
        "\n",
        "results = evaluate_model_comprehensive(c_snn_model, sf_test_dataloader, class_names, device)"
      ],
      "metadata": {
        "id": "xVYO-qo-XTwf"
      },
      "id": "xVYO-qo-XTwf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complexity and Performance Evaluation"
      ],
      "metadata": {
        "id": "LMmJGG1jovYz"
      },
      "id": "LMmJGG1jovYz"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}