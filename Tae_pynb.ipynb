{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJNHMzTmK1825uhtKFb2mS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unipd-human-data/env-soundnet/blob/main/Tae_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "6lGLsxgXZumE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyk12t4HZqTR"
      },
      "outputs": [],
      "source": [
        "!pip install librosa pydub torchaudio snntorch tqdm matplotlib seaborn torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libreria standard\n",
        "import os                               # file e cartelle\n",
        "import numpy as np                      # operazioni matriciali, audio e spet sono matrici\n",
        "import pandas as pd                     # leggere e gestire tabelle come esc50.csv\n",
        "import matplotlib.pyplot as plt         # grafici std\n",
        "import seaborn as sns                   # grafici statistici più belli\n",
        "\n",
        "# Audio\n",
        "import librosa                          # per audio in python, calcola features ecc\n",
        "import librosa.display                  # visualizzare spettrogrammi\n",
        "import torchaudio                       # audio di pytorch, usata per pipeline integrata con PyTorch\n",
        "import torchaudio.transforms as T       # Moduli per convertire audio in MelSpectogram o trasfromazioni\n",
        "from pydub import AudioSegment          # gestire audio a livello più \"umano\"\n",
        "\n",
        "# Deep learning e SNN\n",
        "import torch                            # costruzioni reti neurali\n",
        "import snntorch as snn                  # estensione pytorch per SNN\n",
        "import snntorch.functional as SF\n",
        "from snntorch import spikegen           # converte input in spike train\n",
        "from snntorch import spikeplot as splt\n",
        "import tensorflow as tf                 # costruzione e training di modelli di ml e dl\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchinfo import summary\n",
        "from snntorch import surrogate, functional\n",
        "\n",
        "# Altri\n",
        "from tqdm import tqdm                   # aggiungere barre di progresso ai loop, quanto manca al caricamento audio\n",
        "from torch.utils.data import Dataset    # Dataset class\n",
        "from torch.utils.data import DataLoader # DataLoader class\n",
        "from enum import Enum\n",
        "import time\n",
        "import itertools\n",
        "import json\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "xHDdR5U3Z_E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU disponibile:\", tf.config.list_physical_devices('GPU'))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Userai:\", device)"
      ],
      "metadata": {
        "id": "TIXOTp47aABw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading CSV"
      ],
      "metadata": {
        "id": "pWYMYSUzaBGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "esc_50_df = pd.read_csv(\"/content/drive/MyDrive/HumanData/ESC-50-master/meta/esc50.csv\")\n",
        "\n",
        "def relocate_files(file_name, category, is_esc10):\n",
        "  src = f\"/content/drive/MyDrive/HumanData/ESC-50-master/audio/{file_name}\"\n",
        "  esc50_dest_folder = f\"/content/drive/MyDrive/HumanData/ESC-50/{category}\"\n",
        "  esc10_dest_folder = f\"/content/drive/MyDrive/HumanData/ESC-10/{category}\"\n",
        "\n",
        "  # Ensure destination folder exists, creation folders\n",
        "  os.makedirs(esc50_dest_folder, exist_ok=True)\n",
        "  if is_esc10:\n",
        "    os.makedirs(esc10_dest_folder, exist_ok=True)\n",
        "\n",
        "  dest_esc50 = os.path.join(esc50_dest_folder, file_name)\n",
        "  dest_esc10 = os.path.join(esc10_dest_folder, file_name)\n",
        "\n",
        "  # Check if the file already exists in the destination\n",
        "  if not os.path.exists(dest_esc50):\n",
        "    shutil.copy(src, dest_esc50)\n",
        "    print(f\"Moved {file_name} to {dest_esc50}\")\n",
        "  else:\n",
        "    print(f\"File '{file_name}' already exists in '{dest_esc50}', skipping...\")\n",
        "\n",
        "  if not os.path.exists(dest_esc10):\n",
        "    if is_esc10:\n",
        "      shutil.copy(src, dest_esc10)\n",
        "      print(f\"Moved {file_name} to {dest_esc10}\")\n",
        "  else:\n",
        "    print(f\"File '{file_name}' already exists in '{dest_esc10}', skipping...\")\n",
        "\n",
        "\n",
        "#esc_50_df.apply(lambda row: relocate_files(row['filename'], row['category'], row['esc10']), axis = 1)"
      ],
      "metadata": {
        "id": "63meJT9raC5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls /content/drive/MyDrive/HumanData/ESC-10/"
      ],
      "metadata": {
        "id": "C_C1SmWtaEW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processare tutti i file audio"
      ],
      "metadata": {
        "id": "aJBW_ZqqaGcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/HumanData/ESC-10'\n",
        "SAMPLE_RATE = 44100  # non fare downsampling\n",
        "N_MELS = 128\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 512\n",
        "DURATION = 5.0       # lunghezza standard (secondi)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "class ESCLabels(Enum):\n",
        "    chainsaw = 0\n",
        "    clock_tick = 1\n",
        "    crackling_fire = 2\n",
        "    crying_baby = 3\n",
        "    dog = 4\n",
        "    helicopter = 5\n",
        "    rain = 6\n",
        "    rooster = 7\n",
        "    sea_waves = 8\n",
        "    sneezing = 9"
      ],
      "metadata": {
        "id": "fgZP4Ku6aF30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio_file(file_path, sr=SAMPLE_RATE, duration=5.0, top_db = 30):\n",
        "    y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
        "    #y, _ = librosa.effects.trim(y, top_db=top_db)           # elimina silence iniziale e finale\n",
        "    y = librosa.util.normalize(y)           # normalizzazione RMS\n",
        "    #if len(y) < int(sr * duration):\n",
        "    #    padding = int(sr * duration) - len(y)\n",
        "    #    y = np.pad(y, (0, padding))         # padding se troppo corto\n",
        "    return y"
      ],
      "metadata": {
        "id": "6O1joDJ9aJOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controllo di quanto silenzio tolgo"
      ],
      "metadata": {
        "id": "FapBt6k-aLGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# total_trimmed_start = 0.0\n",
        "# total_trimmed_end = 0.0\n",
        "# clip_count = 0\n",
        "\n",
        "# for label in sorted(os.listdir(BASE_PATH)):\n",
        "#     class_path = os.path.join(BASE_PATH, label)\n",
        "#     for file in os.listdir(class_path):\n",
        "#         if file.endswith(\".wav\"):\n",
        "#             file_path = os.path.join(class_path, file)\n",
        "\n",
        "#             # Carica audio\n",
        "#             y, sr = librosa.load(file_path, sr=44100, duration=5.0)\n",
        "\n",
        "#             # Applica trim\n",
        "#             y_trimmed, index = librosa.effects.trim(y, top_db=30)\n",
        "#             start_sample, end_sample = index\n",
        "\n",
        "#             # Converti in secondi\n",
        "#             start_sec = start_sample / sr\n",
        "#             end_sec = (len(y) - end_sample) / sr\n",
        "\n",
        "#             total_trimmed_start += start_sec\n",
        "#             total_trimmed_end += end_sec\n",
        "#             clip_count += 1\n",
        "\n",
        "#             print(f\"{file} → Trim: start={start_sec:.2f}s, end={end_sec:.2f}s\")\n",
        "\n",
        "# # Statistiche finali\n",
        "# print(\"\\n--- STATISTICHE TOTALI ---\")\n",
        "# print(f\"Clip analizzati: {clip_count}\")\n",
        "# print(f\"Silenzio medio INIZIALE tagliato: {total_trimmed_start / clip_count:.2f} s\")\n",
        "# print(f\"Silenzio medio FINALE tagliato: {total_trimmed_end / clip_count:.2f} s\")\n"
      ],
      "metadata": {
        "id": "78WmqPTQaKGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_audio = []\n",
        "y_labels = []\n",
        "labels = sorted(os.listdir(BASE_PATH))  # lista classi ordinate\n",
        "print(labels, \"\\n\")\n",
        "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "for label in tqdm(labels, desc=\"Caricamento Audio\"):\n",
        "    class_path = os.path.join(BASE_PATH, label)\n",
        "    for file in os.listdir(class_path):\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(class_path, file)\n",
        "            y = load_audio_file(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "            X_audio.append(y)\n",
        "            y_labels.append(label_to_index[label])"
      ],
      "metadata": {
        "id": "jqaFnHXnaOTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X_audio, y_labels, test_size=0.4, random_state=42, stratify=y_labels)\n",
        "\n",
        "# Poi dividi temp in val e test (50/50 => 15% ciascuno)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "NHMpO_7ZaPJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAA"
      ],
      "metadata": {
        "id": "8VAetfbfaWqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def center_crop(signal, target_len):\n",
        "    if len(signal) < target_len:\n",
        "        pad_left = (target_len - len(signal)) // 2\n",
        "        pad_right = target_len - len(signal) - pad_left\n",
        "        return np.pad(signal, (pad_left, pad_right), mode='constant')\n",
        "    else:\n",
        "        start = (len(signal) - target_len) // 2\n",
        "        return signal[start:start + target_len]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XnFvtKlkaY3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def naa(y, sr):\n",
        "    augmented = []\n",
        "    target_len = int(sr * 5.0)\n",
        "\n",
        "    # Originale\n",
        "    augmented.append(center_crop(y, target_len))\n",
        "\n",
        "    # Pitch shift\n",
        "    augmented.append(center_crop(librosa.effects.pitch_shift(y, sr=sr, n_steps=+2), target_len))\n",
        "    augmented.append(center_crop(librosa.effects.pitch_shift(y, sr=sr, n_steps=-2), target_len))\n",
        "\n",
        "    # Time stretch\n",
        "    for rate in [0.7, 1.2]:\n",
        "        y_stretched = librosa.effects.time_stretch(y, rate=rate)\n",
        "        augmented.append(center_crop(y_stretched, target_len))\n",
        "\n",
        "    return augmented\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oM30paSDaZyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "X_train_aug = []\n",
        "y_train_aug = []\n",
        "\n",
        "for i in tqdm(range(len(X_train)), desc=\"NAA\"):\n",
        "    original_audio = X_train[i]\n",
        "    label = y_train[i]\n",
        "\n",
        "    # Applica la tua funzione naa → restituisce 5 versioni (incluso l'originale)\n",
        "    augmented_audios = naa(original_audio, sr=SAMPLE_RATE)\n",
        "\n",
        "    # Aggiungi tutte le versioni alla lista finale\n",
        "    X_train_aug.extend(augmented_audios)\n",
        "    y_train_aug.extend([label] * len(augmented_audios))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sZWDdsoGaa2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Calcola le lunghezze di tutti gli audio\n",
        "lengths = [len(x) for x in X_train_aug]\n",
        "\n",
        "# Trova il minimo e il massimo\n",
        "min_len = min(lengths)\n",
        "max_len = max(lengths)\n",
        "\n",
        "# Converti in secondi (facoltativo)\n",
        "min_sec = min_len / SAMPLE_RATE\n",
        "max_sec = max_len / SAMPLE_RATE\n",
        "\n",
        "print(f\"Audio più corto: {min_len} samples ({min_sec:.2f} s)\")\n",
        "print(f\"Audio più lungo: {max_len} samples ({max_sec:.2f} s)\")\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gQKuGomzabou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converti in log-Mel"
      ],
      "metadata": {
        "id": "Tc723DAFacop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio(y, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
        "  #Calcolare Mel-spectrogram\n",
        "  y_mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "  #Convertire in Log\n",
        "  y_mel = librosa.power_to_db(y_mel, ref=np.max)    #calcola i db rispetto al valore massimo nel Mel-spect.\n",
        "\n",
        "  return y_mel"
      ],
      "metadata": {
        "id": "I2wz3vuLaf6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def clip_silent_frames(mel, threshold_db=-70.0):\n",
        "    silent_mask = np.all(mel < threshold_db, axis=0)\n",
        "    mel[:, silent_mask] = threshold_db  # o np.mean(mel)\n",
        "    return mel\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7bRfRN4tahCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_logmel(X, sr=SAMPLE_RATE):\n",
        "    mel_list = []\n",
        "    for x in tqdm(X, desc=\"Log-Mel\"):\n",
        "        mel = preprocess_audio(x, sr=sr)\n",
        "        # mel = clip_silent_frames(mel)\n",
        "        mel_list.append(mel)\n",
        "    return mel_list"
      ],
      "metadata": {
        "id": "C0s88mX9ahzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "X_train_mel = batch_logmel(X_train_aug)\n",
        "X_val_mel = batch_logmel(X_val)\n",
        "X_test_mel = batch_logmel(X_test)\n",
        "\"\"\"\n",
        "\n",
        "X_train_mel = batch_logmel(X_train)\n",
        "X_val_mel = batch_logmel(X_val)\n",
        "X_test_mel = batch_logmel(X_test)"
      ],
      "metadata": {
        "id": "7IG-LdCxaipz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train set:\", len(X_train_mel))\n",
        "print(\"Val set:\", len(X_val_mel))\n",
        "print(\"Test set:\", len(X_test_mel))\n",
        "\n",
        "print(\"Shape primo sample train:\", X_train_mel[0].shape)"
      ],
      "metadata": {
        "id": "d7-Hanrnajcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mel(mel, title=\"Log-Mel Spectrogram\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mel, sr=SAMPLE_RATE, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualizza un esempio a caso dal training set\n",
        "sample_idx = 32\n",
        "show_mel(X_train_mel[sample_idx], title=\"Esempio dal training set\")\n",
        "print(\"Class: \", ESCLabels(y_train[sample_idx]).name)\n"
      ],
      "metadata": {
        "id": "9ncvVrdeakYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_vals = np.concatenate([mel.flatten() for mel in X_train_mel])\n",
        "print(\"Min:\", np.min(all_vals))\n",
        "print(\"Max:\", np.max(all_vals))"
      ],
      "metadata": {
        "id": "2ei4wEiPalau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def pad_to_multiple_of(mel, multiple=50, value=-80.0):\n",
        "    Pads mel spectrogram along time axis to make time frames a multiple of 'multiple'\n",
        "    current_len = mel.shape[1]\n",
        "    target_len = ((current_len + multiple - 1) // multiple) * multiple\n",
        "    pad_width = target_len - current_len\n",
        "    return np.pad(mel, ((0, 0), (0, pad_width)), mode='constant', constant_values=value)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "3hbkQhfwamLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Applica padding a multiplo di 50 (compatibile con timesteps=50)\n",
        "X_train_mel = [pad_to_multiple_of(mel, multiple=50) for mel in X_train_mel]\n",
        "X_val_mel = [pad_to_multiple_of(mel, multiple=50) for mel in X_val_mel]\n",
        "X_test_mel = [pad_to_multiple_of(mel, multiple=50) for mel in X_test_mel]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yDtm_-gVam8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train shape esempio:\", X_train_mel[0].shape)\n",
        "print(\"Val shape esempio:\", X_val_mel[0].shape)\n",
        "print(\"Test shape esempio:\", X_test_mel[0].shape)"
      ],
      "metadata": {
        "id": "8kFqfmFzan5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_mel_np = np.array(X_train_mel)\n",
        "X_val_mel_np = np.array(X_val_mel)\n",
        "X_test_mel_np = np.array(X_test_mel)\n",
        "\n",
        "if len(X_train_mel_np.shape) == 3:\n",
        "    X_train_mel_np = X_train_mel_np[..., np.newaxis]  # (N, 64, 431, 1)\n",
        "\n",
        "#y_train_np = np.array(y_train_aug)  # Etichette corrispondenti\n",
        "y_train_np = np.array(y_train)"
      ],
      "metadata": {
        "id": "2tvzHFGTasXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizzazione"
      ],
      "metadata": {
        "id": "2EftuVULatYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_mel_np = (X_train_mel_np + 80.0) / 80.0\n",
        "X_val_mel_np = (X_val_mel_np + 80.0) / 80.0\n",
        "X_test_mel_np = (X_test_mel_np + 80.0) / 80.0"
      ],
      "metadata": {
        "id": "zyNHb3j-avQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TAA"
      ],
      "metadata": {
        "id": "Zoy5-Za3ao4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "taa_generator = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.25,\n",
        "    shear_range=0.3,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IaWdmWZAarcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "X_taa = []\n",
        "y_taa = []\n",
        "\n",
        "augmentations_per_sample = 4  # puoi aumentare questo numero\n",
        "\n",
        "for i in tqdm(range(len(X_train_mel_np)), desc=\"TAA Augmentation\"):\n",
        "    sample = X_train_mel_np[i]  # shape: (64, max_len, 1)\n",
        "    sample = np.expand_dims(sample, axis=0)  # shape: (1, 64, max_len, 1)\n",
        "\n",
        "    # Genera augmentazioni\n",
        "    gen = taa_generator.flow(sample, batch_size=1)\n",
        "    for _ in range(augmentations_per_sample):\n",
        "        aug_sample = next(gen)[0]  # shape: (64, max_len, 1)\n",
        "        X_taa.append(aug_sample)\n",
        "        y_taa.append(y_train_np[i])\n",
        "\n",
        "# Combina con il training set originale\n",
        "X_train_augmented = np.concatenate([X_train_mel_np, np.array(X_taa)], axis=0)\n",
        "y_train_augmented = np.concatenate([y_train_np, np.array(y_taa)], axis=0)\n",
        "\n",
        "print(\"Nuova shape X:\", X_train_augmented.shape)\n",
        "print(\"Nuova shape y:\", y_train_augmented.shape)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lTbm4UkPawIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converto in tensori"
      ],
      "metadata": {
        "id": "oq0vJyG3a3Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "X_train = torch.from_numpy(X_train_augmented).float()\n",
        "X_val = torch.from_numpy(X_val_mel_np).float()\n",
        "X_test = torch.from_numpy(X_test_mel_np).float()\n",
        "y_train = torch.from_numpy(y_train_augmented).long()\n",
        "y_val = torch.from_numpy(np.array(y_val)).long()\n",
        "y_test = torch.from_numpy(np.array(y_test)).long()\n",
        "\"\"\"\n",
        "\n",
        "X_train = torch.from_numpy(X_train_mel_np).float()\n",
        "X_val = torch.from_numpy(X_val_mel_np).float()\n",
        "X_test = torch.from_numpy(X_test_mel_np).float()\n",
        "y_train = torch.from_numpy(y_train_np).long()\n",
        "y_val = torch.from_numpy(np.array(y_val)).long()\n",
        "y_test = torch.from_numpy(np.array(y_test)).long()"
      ],
      "metadata": {
        "id": "oHtz9uF2a4pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape X_train:\", X_train.shape)\n",
        "print(\"Tipo dati:\", X_train.dtype)\n",
        "print(\"Valori min/max:\", torch.min(X_train), torch.max(X_train))"
      ],
      "metadata": {
        "id": "sXGiRXDTa5nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mel_sample(index):\n",
        "    mel = X_train[index].numpy().squeeze()  # converti tensore in NumPy\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mel, sr=SAMPLE_RATE, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar()  # non usare '%+2.0f dB' perché ora i valori sono [0, 1]\n",
        "    plt.title(f\"Log-Mel sample #{index} (normalized)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"Class:\", ESCLabels(y_train[index].item()).name)\n",
        "\n",
        "for i in range(3):\n",
        "    show_mel_sample(i)"
      ],
      "metadata": {
        "id": "dm91w_GKa6nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"Etichette uniche:\", np.unique(y_train))\n",
        "\n",
        "# Stampa etichetta di un esempio\n",
        "for i in range(3):\n",
        "    print(f\"Esempio {i}: label = {y_train[i]}\")ù"
      ],
      "metadata": {
        "id": "fyemSqsJa8SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spike Encoding"
      ],
      "metadata": {
        "id": "Z9sLwJSra844"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_spike_trains(spike_tensor, sample_idx=0, mel_bin=10):\n",
        "    \"\"\"\n",
        "    Visualizes delta spike encodings with three plots:\n",
        "    1. Full raster plot of all mel bins\n",
        "    2. Single mel bin spike train with vertical lines\n",
        "    3. Density plot showing balance between positive and negative spikes\n",
        "\n",
        "    Args:\n",
        "      spike_tensor : torch.Tensor or list\n",
        "          The delta spike tensor. Expected shape: [batch_size, time_frames, n_mels]\n",
        "          If from DataLoader, expected to be a list containing tensors\n",
        "      sample_idx : int\n",
        "          Index of the sample in the batch to visualize\n",
        "      mel_bin : int\n",
        "          Which mel frequency bin to visualize in the spike train plot\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if input is a list (from DataLoader) or direct tensor\n",
        "    if isinstance(spike_tensor, list):\n",
        "        spikes = spike_tensor[0].squeeze(-1)[sample_idx].detach().cpu().numpy()\n",
        "    elif isinstance(spike_tensor, torch.Tensor):\n",
        "        # If 4D tensor [batch_size, time_frames, n_mels]\n",
        "        if len(spike_tensor.shape) == 3:\n",
        "            spikes = spike_tensor.squeeze(-1)[sample_idx, :, :].detach().cpu().numpy()\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected spike tensor shape: {spike_tensor.shape}\")\n",
        "    else:\n",
        "        raise TypeError(\"spike_tensor must be a torch.Tensor or a list containing tensors\")\n",
        "\n",
        "    if len(spikes.shape) > 2:\n",
        "        spikes = spikes.squeeze()\n",
        "\n",
        "    pos_spikes = (spikes > 0).astype(float)\n",
        "    neg_spikes = (spikes < 0).astype(float)\n",
        "\n",
        "    plt.figure(figsize=(10, 12))\n",
        "\n",
        "    # 1. Raster Plot with Inverted Y-axis\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.imshow(spikes.T, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1, origin='lower')\n",
        "    plt.colorbar(ticks=[-1, 0, 1], label='Spike Type')\n",
        "    plt.title(\"Full Raster Plot\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Mel Bin\")\n",
        "    num_mel_bins = spikes.shape[1]\n",
        "    tick_interval = max(1, num_mel_bins // 7)\n",
        "    plt.yticks(np.arange(0, num_mel_bins, tick_interval))\n",
        "\n",
        "    # 2. Single Mel Bin Spike Train with vlines\n",
        "    plt.subplot(3, 1, 2)\n",
        "    pos_times = np.where(spikes[:, mel_bin] == 1)[0]\n",
        "    neg_times = np.where(spikes[:, mel_bin] == -1)[0]\n",
        "    plt.vlines(pos_times, 0, 1, color='red', linewidth=0.8)\n",
        "    plt.vlines(neg_times, -1, 0, color='blue', linewidth=0.8)\n",
        "    plt.yticks([-1, 0, 1])\n",
        "    plt.ylim(-1.2, 1.2)\n",
        "    plt.title(f\"Spike Train for Mel Bin {mel_bin}\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Spike Value\")\n",
        "    plt.grid(False)\n",
        "    legend_elements = [plt.Line2D([0], [0], color='red', lw=2, label='Positive Spikes'),\n",
        "                      plt.Line2D([0], [0], color='blue', lw=2, label='Negative Spikes')]\n",
        "    plt.legend(handles=legend_elements)\n",
        "\n",
        "    # 3. Density Plot with Legend\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.stackplot(np.arange(spikes.shape[0]),\n",
        "                  pos_spikes.sum(axis=1),\n",
        "                  -neg_spikes.sum(axis=1),\n",
        "                  colors=['red', 'blue'])\n",
        "\n",
        "    plt.legend(['Positive Spikes', 'Negative Spikes'])\n",
        "    plt.title(\"Spike Polarity Balance Over Time\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Net Spike Count\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5baspTjRbAZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DeltaAudioDataset(Dataset):\n",
        "    def __init__(self, X, y, threshold=0.05, timesteps=10, off_spike=True):\n",
        "        \"\"\"\n",
        "        X: Tensor of shape [num_samples, n_mels, time_steps, 1]\n",
        "        y: Tensor of shape [num_samples]\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.timesteps = timesteps\n",
        "        self.threshold = threshold\n",
        "        self.off_spike = off_spike\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx].squeeze(-1)  # [n_mels, time_steps]\n",
        "        y = self.y[idx]\n",
        "\n",
        "        chunks = torch.tensor_split(x, self.timesteps, dim=-1)\n",
        "\n",
        "        x_chunks = torch.stack(\n",
        "            [window.mean(dim=-1) for window in chunks], dim=0\n",
        "        )\n",
        "\n",
        "        # Apply delta modulation (shape stays [time_steps, n_mels])\n",
        "        spike_train = spikegen.delta(\n",
        "            x_chunks, threshold=self.threshold, off_spike=self.off_spike\n",
        "        )\n",
        "\n",
        "        return spike_train, y.long()\n"
      ],
      "metadata": {
        "id": "Rb8u73XlboZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TAE"
      ],
      "metadata": {
        "id": "oJuRL4HlbBVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ThresholdAdaptiveEncoding(Dataset):\n",
        "\n",
        "  def __init__(self, X, y, alpha=0.9, threshold=0.05):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.alpha = alpha\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = self.X[idx].squeeze(-1)  # [n_mels, time_steps]\n",
        "    y = self.y[idx]\n",
        "\n",
        "    spike_train = torch.zeros(\n",
        "        x.shape[1], x.shape[0], dtype=torch.int8\n",
        "    )  # [time_steps, n_mels]\n",
        "\n",
        "    for mel_idx in range(x.shape[0]):\n",
        "      spike_train[:, mel_idx] = self._encode_tae(x[mel_idx])\n",
        "\n",
        "    return spike_train, y.long()\n",
        "\n",
        "  def _encode_tae(self, signal):\n",
        "    \"\"\"\n",
        "    Encode a signal using the Threshold Adaptive Encoding (TAE) algorithm.\n",
        "\n",
        "    Args:\n",
        "        signal: 1D tensor or 2D tensor of shape [1, N]\n",
        "\n",
        "    Returns:\n",
        "        1D tensor of encoded spikes with values in {-1, 0, 1}\n",
        "    \"\"\"\n",
        "    if signal.dim() == 2:\n",
        "        signal = signal.squeeze(0)  # ensure shape [N]\n",
        "\n",
        "    signal_np = signal.numpy()\n",
        "    spikes = np.zeros_like(signal_np, dtype=np.int8)\n",
        "\n",
        "    base = signal_np[0]\n",
        "    threshold = self.threshold  # use instance value\n",
        "    alpha = self.alpha          # decay factor\n",
        "\n",
        "    for i in range(1, len(signal_np)):\n",
        "        diff = signal_np[i] - base\n",
        "        if diff >= threshold:\n",
        "            spikes[i] = 1\n",
        "            base += threshold\n",
        "            threshold *= alpha\n",
        "        elif diff <= -threshold:\n",
        "            spikes[i] = -1\n",
        "            base -= threshold\n",
        "            threshold *= alpha\n",
        "        else:\n",
        "            threshold *= alpha\n",
        "\n",
        "    return torch.from_numpy(spikes)\n",
        "\n"
      ],
      "metadata": {
        "id": "ok-E5EDZbRfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPHA = 1.0\n",
        "\n",
        "tae_train_dataset = ThresholdAdaptiveEncoding(\n",
        "    X_train, y_train,\n",
        "    alpha=ALPHA\n",
        ")\n",
        "\n",
        "tae_val_dataset = ThresholdAdaptiveEncoding(\n",
        "    X_val, y_val,\n",
        "    alpha=ALPHA\n",
        ")\n",
        "\n",
        "tae_test_dataset = ThresholdAdaptiveEncoding(\n",
        "    X_test, y_test,\n",
        "    alpha=ALPHA\n",
        ")\n",
        "\n",
        "# Create TAE dataloaders\n",
        "tae_train_dataloader = DataLoader(\n",
        "    dataset=tae_train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "tae_val_dataloader = DataLoader(\n",
        "    dataset=tae_val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "tae_test_dataloader = DataLoader(\n",
        "    dataset=tae_test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "# Test the TAE encoding\n",
        "print(\"Testing Threshold Adaptive Encoding...\")\n",
        "tae_x_batch, tae_y_batch = next(iter(tae_train_dataloader))\n",
        "print(f\"TAE batch shape: {tae_x_batch.shape}\")\n"
      ],
      "metadata": {
        "id": "YHT4CGg1bSm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 10\n",
        "visualize_spike_trains(spike_tensor=tae_x_batch, sample_idx=sample_idx, mel_bin=10)\n",
        "print(f\"TAE Class: {ESCLabels(tae_y_batch[sample_idx].item()).name}\")"
      ],
      "metadata": {
        "id": "2w2EpcTzbT4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SNN Modelling"
      ],
      "metadata": {
        "id": "2scK2xUabWGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SNNClassifier(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_mels,\n",
        "        hidden_sizes,\n",
        "        num_classes,\n",
        "        surr_grad,\n",
        "        learn_thr=True,\n",
        "        learn_beta=True,\n",
        "    ):\n",
        "        super(SNNClassifier, self).__init__()\n",
        "        self.n_mels = n_mels\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.num_classes = num_classes\n",
        "        self.surr_grad = surr_grad\n",
        "        self.learn_thr = learn_thr\n",
        "        self.learn_beta = learn_beta\n",
        "\n",
        "        # Layer 1: Input to Hidden 1\n",
        "        self.fc1 = torch.nn.Linear(n_mels, self.hidden_sizes[0])\n",
        "        self.lif1 = snn.Leaky(\n",
        "            beta=torch.full((self.hidden_sizes[0],), 0.5),\n",
        "            learn_beta=learn_beta,\n",
        "            learn_threshold=learn_thr,\n",
        "            spike_grad=surr_grad,\n",
        "            reset_mechanism=\"zero\",\n",
        "        )\n",
        "\n",
        "        # Layer 2: Hidden 1 to Hidden 2\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_sizes[0], self.hidden_sizes[1])\n",
        "        self.lif2 = snn.Leaky(\n",
        "            beta=torch.full((self.hidden_sizes[1],), 0.5),\n",
        "            learn_beta=learn_beta,\n",
        "            learn_threshold=learn_thr,\n",
        "            spike_grad=surr_grad,\n",
        "            reset_mechanism=\"zero\",\n",
        "        )\n",
        "\n",
        "        # Layer 3: Hidden 2 to Hidden 3\n",
        "        self.fc3 = torch.nn.Linear(self.hidden_sizes[1], self.hidden_sizes[2])\n",
        "        self.lif3 = snn.Leaky(\n",
        "            beta=torch.full((self.hidden_sizes[2],), 0.5),\n",
        "            learn_beta=learn_beta,\n",
        "            learn_threshold=learn_thr,\n",
        "            spike_grad=surr_grad,\n",
        "            reset_mechanism=\"zero\",\n",
        "        )\n",
        "\n",
        "        # Output Layer\n",
        "        self.fc_out = torch.nn.Linear(self.hidden_sizes[2], num_classes)\n",
        "        self.lif_out = snn.Leaky(\n",
        "            beta=torch.full((num_classes,), 0.5),\n",
        "            learn_beta=learn_beta,\n",
        "            learn_threshold=learn_thr,\n",
        "            spike_grad=surr_grad,\n",
        "            reset_mechanism='none',\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, time_steps, _ = x.shape\n",
        "\n",
        "        # Initialize membrane potentials\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem_out = self.lif_out.init_leaky()\n",
        "\n",
        "        spk_rec = []\n",
        "        mem_rec = []\n",
        "\n",
        "        for step in range(time_steps):\n",
        "            x_t = x[:, step, :]\n",
        "\n",
        "            cur1 = self.fc1(x_t)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "\n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            cur_out = self.fc_out(spk3)\n",
        "            spk_out, mem_out = self.lif_out(cur_out, mem_out)\n",
        "\n",
        "            spk_rec.append(spk_out)\n",
        "            mem_rec.append(mem_out)\n",
        "\n",
        "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)\n"
      ],
      "metadata": {
        "id": "V27PruDKbYzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "OQeYJ05FbgEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, train_loader, valid_loader, accuracy, loss_fn, optimizer,\n",
        "             epochs, patience, path, verbose=True, max_batches=None):\n",
        "    \"\"\"\n",
        "    Optimized training function with:\n",
        "    - Optional batch limiting (for quick testing)\n",
        "    - Mixed precision training\n",
        "    - Reduced GPU memory usage\n",
        "    - Progress tracking\n",
        "    \"\"\"\n",
        "    train_loss_list, val_loss_list = [], []\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "    counter = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Enable mixed precision training if available\n",
        "    scaler = torch.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Training mode\n",
        "        model.train()\n",
        "        train_loss, train_acc = 0.0, 0.0\n",
        "\n",
        "        for batch_idx, (X, y) in enumerate(tqdm(train_loader, desc=\"Train batches\", leave=False, total=len(train_loader))):\n",
        "            X = X.squeeze().to(device)\n",
        "            y = y.squeeze().long().to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Use mixed precision where available\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    spk_out, _ = model(X.float())\n",
        "                    acc = accuracy(spk_out, y)\n",
        "                    loss = loss_fn(spk_out, y)\n",
        "\n",
        "                # Scale gradients and optimize\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                spk_out, _ = model(X.float())\n",
        "                acc = accuracy(spk_out, y)\n",
        "                loss = loss_fn(spk_out, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            train_acc += acc.item()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Free up memory\n",
        "            del X, y, spk_out, loss, acc\n",
        "\n",
        "        # Calculate average metrics\n",
        "        train_loss_avg = train_loss / len(train_loader)\n",
        "        train_acc_avg = train_acc / len(train_loader)\n",
        "        train_loss_list.append(train_loss_avg)\n",
        "        train_acc_list.append(train_acc_avg)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_acc = 0.0, 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (X, y) in enumerate(valid_loader):\n",
        "                X = X.squeeze().to(device)\n",
        "                y = y.squeeze().long().to(device)\n",
        "\n",
        "                # Forward pass with reduced memory usage\n",
        "                if scaler:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        spk_out, _ = model(X.float())\n",
        "                        acc = accuracy(spk_out, y)\n",
        "                        loss = loss_fn(spk_out, y)\n",
        "                else:\n",
        "                    spk_out, _ = model(X.float())\n",
        "                    acc = accuracy(spk_out, y)\n",
        "                    loss = loss_fn(spk_out, y)\n",
        "\n",
        "                val_acc += acc.item()\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Free up memory\n",
        "                del X, y, spk_out, loss, acc\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss_avg = val_loss / len(valid_loader)\n",
        "        val_acc_avg = val_acc / len(valid_loader)\n",
        "        val_loss_list.append(val_loss_avg)\n",
        "        val_acc_list.append(val_acc_avg)\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_loss_avg < best_val_loss:\n",
        "            best_val_loss = val_loss_avg\n",
        "            counter = 0\n",
        "            if path:\n",
        "                #torch.save(model.state_dict(), path)\n",
        "                pass\n",
        "        else:\n",
        "            counter += 1\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        end_time = time.time()\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs} - {int(end_time-start_time)}s - \"\n",
        "                  f\"loss: {train_loss_avg:.4f} - acc: {train_acc_avg:.4f} - \"\n",
        "                  f\"val_loss: {val_loss_avg:.4f} - val_acc: {val_acc_avg:.4f}\")\n",
        "\n",
        "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
      ],
      "metadata": {
        "id": "wwZxBk75byEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalutation"
      ],
      "metadata": {
        "id": "mmQzOBQ1bzQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Valutazione nel test set\n",
        "def evaluate_model_comprehensive(model, dataloader, class_names, device):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation function that computes accuracy, F1, precision, and recall.\n",
        "    Now handles zero-division gracefully and reports classes with no predictions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.squeeze().to(device)\n",
        "            y = y.squeeze().long().to(device)\n",
        "\n",
        "            spk_out, _ = model(X.float())\n",
        "            spk_sum = spk_out.sum(dim=0)  # [batch_size, num_classes]\n",
        "            preds = torch.argmax(spk_sum, dim=1)\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(y.cpu())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    # Identify classes never predicted\n",
        "    missing = set(range(len(class_names))) - set(np.unique(all_preds))\n",
        "    if missing:\n",
        "        print(\"WARNING: The following classes were never predicted:\",\n",
        "              [class_names[i] for i in sorted(missing)])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "\n",
        "    # Macro and weighted metrics with zero_division=0\n",
        "    f1_macro    = f1_score(all_labels, all_preds, average='macro',    zero_division=0)\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    precision_macro    = precision_score(all_labels, all_preds, average='macro',    zero_division=0)\n",
        "    precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    recall_macro    = recall_score(all_labels, all_preds, average='macro',    zero_division=0)\n",
        "    recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    # Per-class metrics\n",
        "    f1_per_class        = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    recall_per_class    = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"=\"*60)\n",
        "    print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\\n\")\n",
        "\n",
        "    print(\"MACRO AVERAGES:\")\n",
        "    print(f\"  F1-Score   : {f1_macro:.4f}\")\n",
        "    print(f\"  Precision  : {precision_macro:.4f}\")\n",
        "    print(f\"  Recall     : {recall_macro:.4f}\\n\")\n",
        "\n",
        "    print(\"WEIGHTED AVERAGES:\")\n",
        "    print(f\"  F1-Score   : {f1_weighted:.4f}\")\n",
        "    print(f\"  Precision  : {precision_weighted:.4f}\")\n",
        "    print(f\"  Recall     : {recall_weighted:.4f}\\n\")\n",
        "\n",
        "    print(\"PER-CLASS METRICS:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Class':<15} {'F1':<8} {'Precision':<10} {'Recall':<8}\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"{name:<15} \"\n",
        "              f\"{f1_per_class[i]:<8.4f} \"\n",
        "              f\"{precision_per_class[i]:<10.4f} \"\n",
        "              f\"{recall_per_class[i]:<8.4f}\")\n",
        "\n",
        "    # Detailed report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        target_names=class_names,\n",
        "        digits=4,\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation=45)\n",
        "    plt.title(\"Confusion Matrix - Delta Modulation SNN\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, f1_macro, f1_weighted, precision_macro, precision_weighted, recall_macro, recall_weighted, f1_per_class, precision_per_class, recall_per_class, cm"
      ],
      "metadata": {
        "id": "NCYTg3pXbyxI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}